{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "bqUKpxrLsDdp"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  code by Tae Hwan Jung(Jeff Jung) @graykode, modify by wmathor\n",
        "'''\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optimizer\n",
        "import torch.utils.data as Data\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dtype = torch.FloatTensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "OteNgpGiuIAL"
      },
      "outputs": [],
      "source": [
        "sentences = [\"jack like dog\", \"jack like cat\", \"jack like animal\",\n",
        "  \"dog cat animal\", \"banana apple cat dog like\", \"dog fish milk like\",\n",
        "  \"dog cat animal like\", \"jack like apple\", \"apple like\", \"jack like banana\",\n",
        "  \"apple banana jack movie book music like\", \"cat dog hate\", \"cat dog like\"]\n",
        "sentence_list = \" \".join(sentences).split() # ['jack', 'like', 'dog']\n",
        "vocab = list(set(sentence_list))\n",
        "word2idx = {w:i for i, w in enumerate(vocab)}\n",
        "vocab_size = len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "qP95fgZduiQ5"
      },
      "outputs": [],
      "source": [
        "# model parameters\n",
        "C = 2 # window size 中心词为原点左右各看C个单词算背景词，这是一个组一起预测出来\n",
        "batch_size = 8\n",
        "m = 2 # word embedding dim, 词向量表的列数，行数就是词库的个数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "fpvih6WlvFib"
      },
      "outputs": [],
      "source": [
        "skip_grams = [] # 这是一个组skip_grams=中心词center+左右的背景词context\n",
        "for idx in range(C, len(sentence_list) - C): # 从c到len()-c, 保证有context\n",
        "  center = word2idx[sentence_list[idx]]\n",
        "  context_idx = list(range(idx - C, idx)) + list(range(idx + 1, idx + C + 1))\n",
        "  context = [word2idx[sentence_list[i]] for i in context_idx]\n",
        "\n",
        "  for w in context:\n",
        "    skip_grams.append([center, w])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "fSG4Hox3wv0b"
      },
      "outputs": [],
      "source": [
        "def make_data(skip_grams):\n",
        "  input_data = []\n",
        "  output_data = []\n",
        "  for a, b in skip_grams:\n",
        "    input_data.append(np.eye(vocab_size)[a]) #用one-hot vector来编码，一次取单位矩阵的一行\n",
        "    output_data.append(b)\n",
        "  return input_data, output_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "SNGvrTT2xmyE"
      },
      "outputs": [],
      "source": [
        "input_data, output_data = make_data(skip_grams)\n",
        "input_data, output_data = torch.Tensor(input_data), torch.LongTensor(output_data)\n",
        "dataset = Data.TensorDataset(input_data, output_data)\n",
        "loader = Data.DataLoader(dataset, batch_size, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "W6YsUmxzx3SZ"
      },
      "outputs": [],
      "source": [
        "class Word2Vec(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Word2Vec, self).__init__()\n",
        "    self.W = nn.Parameter(torch.randn(vocab_size, m).type(dtype))\n",
        "    self.V = nn.Parameter(torch.randn(m, vocab_size).type(dtype))\n",
        "\n",
        "  def forward(self, X):\n",
        "    # X : [batch_size, vocab_size]\n",
        "    hidden = torch.mm(X, self.W) # [batch_size, m]\n",
        "    output = torch.mm(hidden, self.V) # [batch_size, vocab_size]\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "qTrVrbNsy0bN"
      },
      "outputs": [],
      "source": [
        "model = Word2Vec().to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()#.to(device) #这个应该不用to(device)\n",
        "optim = optimizer.Adam(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "colab_type": "code",
        "id": "zDcjLp7Ty2Qs",
        "outputId": "bfda288b-08e2-42fc-9753-19657c7d097b"
      },
      "outputs": [],
      "source": [
        "for epoch in range(10):\n",
        "  for i, (batch_x, batch_y) in enumerate(loader):\n",
        "    batch_x = batch_x.to(device)\n",
        "    batch_y = batch_y.to(device)\n",
        "    pred = model(batch_x)\n",
        "    loss = loss_fn(pred, batch_y)\n",
        "\n",
        "    if (epoch + 1) % 1000 == 0:\n",
        "      print(epoch + 1, i, loss.item())\n",
        "    \n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "colab_type": "code",
        "id": "9vT14L_izbWC",
        "outputId": "fdb89ebf-dabe-4516-caa9-976e0ec4ac4c"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "for i, label in enumerate(vocab):\n",
        "  W, WT = model.parameters()\n",
        "  x,y = float(W[i][0]), float(W[i][1])\n",
        "  plt.scatter(x, y)\n",
        "  plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "D_5Cauheztue"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "“Word2Vec-Torch(Softmax).ipynb”的副本",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.11 ('week6')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.11"
    },
    "vscode": {
      "interpreter": {
        "hash": "dc22443302a7e1520fd496d19808d411f95cd97f052bed8809baa60e4c089c3b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "7-FP_Q8vJUtX"
      },
      "outputs": [],
      "source": [
        "# code by Tae Hwan Jung @graykode, modify by wmathor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optimizer\n",
        "import torch.utils.data as Data\n",
        "\n",
        "dtype = torch.FloatTensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "5K5cfsDwP6JQ"
      },
      "outputs": [],
      "source": [
        "sentences = ['i like cat', 'i love coffee', 'i hate milk']\n",
        "sentences_list = \" \".join(sentences).split() #从左到右运行 ['i', 'like', 'cat', 'i', 'love'. 'coffee',...]\n",
        "vocab = list(set(sentences_list))\n",
        "word2idx = {w:i for i, w in enumerate(vocab)}\n",
        "idx2word = {i:w for i, w in enumerate(vocab)}\n",
        "\n",
        "V = len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "dzkc00uqQiXT"
      },
      "outputs": [],
      "source": [
        "def make_data(sentences):\n",
        "  input_data = []\n",
        "  target_data = []\n",
        "  for sen in sentences:\n",
        "    sen = sen.split() # ['i', 'like', 'cat']\n",
        "    input_tmp = [word2idx[w] for w in sen[:-1]]\n",
        "    target_tmp = word2idx[sen[-1]]\n",
        "\n",
        "    input_data.append(input_tmp)\n",
        "    target_data.append(target_tmp)\n",
        "  return input_data, target_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "2kBFTErzRMFy"
      },
      "outputs": [],
      "source": [
        "input_data, target_data = make_data(sentences)\n",
        "input_data, target_data = torch.LongTensor(input_data), torch.LongTensor(target_data)\n",
        "dataset = Data.TensorDataset(input_data, target_data)\n",
        "loader = Data.DataLoader(dataset, 2, True) #要tensor, batchsize,shuffle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "XlQ46HoTRyxn"
      },
      "outputs": [],
      "source": [
        "# parameters\n",
        "m = 2\n",
        "n_step = 2\n",
        "n_hidden = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "uGURCnXxRav9"
      },
      "outputs": [],
      "source": [
        "class NNLM(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NNLM, self).__init__() #self转成NNLM的爸爸的类型nn.Module\n",
        "    self.C = nn.Embedding(V, m) #词向量索引表\n",
        "    # 下面这个维度是对着forward里面的公式填的\n",
        "    self.H = nn.Parameter(torch.randn(n_step * m, n_hidden).type(dtype)) #tanh(X*H+d)\n",
        "    self.d = nn.Parameter(torch.randn(n_hidden).type(dtype)) #隐藏层\n",
        "    \n",
        "    self.b = nn.Parameter(torch.randn(V).type(dtype))\n",
        "    self.U = nn.Parameter(torch.randn(n_hidden, V).type(dtype)) #输出层 输出V个单词每个的概率，取max的那个\n",
        "    self.W = nn.Parameter(torch.randn(n_step * m, V).type(dtype)) #residue操作的对输入乘\n",
        "\n",
        "  def forward(self, X):\n",
        "    '''\n",
        "    X : [batch_size, n_step] 这里（打包2句, 看历史2个单词）所以:[2,2]\n",
        "    '''\n",
        "    X = self.C(X) # 用C这个索引表一照以后，多了m这维，自己定的： [batch_size, n_step, m]\n",
        "    # print(X)\n",
        "    X = X.view(-1, n_step * m) # [batch_szie, n_step * m] \n",
        "    hidden_out = torch.tanh(self.d + torch.mm(X, self.H)) # [batch_size, n_hidden]\n",
        "    output = self.b + torch.mm(X, self.W) + torch.mm(hidden_out, self.U) #nn.CrossEntropyLoss中已经实现了softmax功能，因此在分类任务的最后一层fc后不需要加入softmax激活函数\n",
        "    return output\n",
        "model = NNLM()\n",
        "optim = optimizer.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "colab_type": "code",
        "id": "dtjKlEU-UP_l",
        "outputId": "759c1167-9a5c-4d1d-d621-2efb0e626454"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000 0.008951702155172825\n",
            "1000 0.025303568691015244\n",
            "2000 0.002477166010066867\n",
            "2000 0.002058174693956971\n",
            "3000 0.0004532400635071099\n",
            "3000 0.0010829067323356867\n",
            "4000 0.00027020866400562227\n",
            "4000 0.00011598391574807465\n",
            "5000 5.781473373644985e-05\n",
            "5000 0.00011276562872808427\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(5000): #一个epoch看完所有batches\n",
        "  for batch_x, batch_y in loader:\n",
        "    pred = model(batch_x)\n",
        "    loss = criterion(pred, batch_y)\n",
        "\n",
        "    if (epoch + 1) % 1000 == 0:\n",
        "      print(epoch + 1, loss.item())\n",
        "    \n",
        "    optim.zero_grad() #三件套，优化器迭代的初始化设成0\n",
        "    loss.backward()\n",
        "    optim.step() #更新parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "0Mqqhs4-UpaB",
        "outputId": "87c8a8c0-2413-4d78-8818-dc8f25e03682"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['cat', 'coffee', 'milk']\n"
          ]
        }
      ],
      "source": [
        "# Pred\n",
        "pred = model(input_data).max(1, keepdim=True)[1] #model(input_data)三句话，每句话输出V个单词的概率值，取max,输出[0]是值，[1]是indices\n",
        "print([idx2word[idx.item()] for idx in pred.squeeze()]) #要加list[] 用indices去找word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2, 0, 3]\n",
            "[tensor(2), tensor(0), tensor(3)]\n",
            "['cat', 'coffee', 'milk']\n"
          ]
        }
      ],
      "source": [
        "pred=model(input_data).max(1)[1]\n",
        "print([idx.item() for idx in pred])\n",
        "print([idx for idx in pred])\n",
        "print([idx2word[idx.item()] for idx in pred])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "“NNLM-Torch.ipynb”的副本",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.11 ('week6')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.11"
    },
    "vscode": {
      "interpreter": {
        "hash": "dc22443302a7e1520fd496d19808d411f95cd97f052bed8809baa60e4c089c3b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

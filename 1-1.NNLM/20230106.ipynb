{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import torch.nn as nn\n",
    "dtype=torch.FloatTensor #32位"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、建索引表\n",
    "\n",
    "1.1句子拆成单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'like', 'cat', 'i', 'love', 'coffee', 'i', 'hate', 'milk']\n"
     ]
    }
   ],
   "source": [
    "sentences=['i like cat', 'i love coffee', 'i hate milk']\n",
    "# sentences=[i.split() for i in sentences] #.split只能对str用# sentences=''.join('ab c').split()# sentences=''.join(sentences) #句子之间就没加空格，有空格不会再加空格# sentences.split()\n",
    "sentences_list=' '.join(sentences).split() #.join .split是str操作 默认通过空格分开单词\n",
    "print(sentences_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 建词单词的索引表 用字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=list(set(sentences_list)) #去重用set 因为集合无重复元素\n",
    "word2idx={w:i for i,w in enumerate(vocab)} #妙啊enumerate\n",
    "idx2word={i:w for i,w in enumerate(vocab)}\n",
    "V=len(vocab)\n",
    "m=2 #词向量表的每一个单词的表示\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 准备X,y 转成数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makedata(sentences):\n",
    "    input,output=[],[]\n",
    "    for sen in sentences:\n",
    "        sen=sen.split()\n",
    "        input_tmp=[word2idx[w] for w in sen]\n",
    "        output_tmp=word2idx[sen[-1]]\n",
    "\n",
    "        input.append(input_tmp)\n",
    "        output.append(output_tmp)\n",
    "    return input,output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input,output=makedata(sentences)\n",
    "input,output=torch.LongTensor(input),torch.LongTensor(output)\n",
    "dataset=TensorDataset(input,output)\n",
    "loader=DataLoader(dataset,2,True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=2\n",
    "n_step=2\n",
    "n_hidden=10\n",
    "class NNLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNLM,self).__init__()\n",
    "        self.C=nn.Embedding(V,m)\n",
    "        self.H=nn.Parameter(torch.randn(n_step*m,n_hidden).type(dtype))\n",
    "        self.d=nn.Parameter(torch.randn(n_hidden).type(dtype))\n",
    "        self.U=nn.Parameter(torch.randn(n_hidden,V).type(dtype))\n",
    "        self.b=nn.Parameter(torch.randn(V).type(dtype)) \n",
    "        self.W=nn.Parameter(torch.randn(n_step*m,V).type(dtype))      \n",
    "    \n",
    "    def forward(self,X):\n",
    "        X=self.C(X)\n",
    "        # print(X.shape)\n",
    "        X=X.view(-1,n_step*m)\n",
    "        hidden_out=torch.tanh(torch.mm(X,self.H)+self.d)\n",
    "        output=torch.mm(X,self.W)+torch.mm(hidden_out,self.U)+self.b\n",
    "        # output=torch.softmax(output) \n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (3) to match target batch_size (2).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\机器学习\\nlp-tutorial\\1-1.NNLM\\20230106.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/nlp-tutorial/1-1.NNLM/20230106.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_x,batch_y \u001b[39min\u001b[39;00m loader:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/nlp-tutorial/1-1.NNLM/20230106.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     pred\u001b[39m=\u001b[39mmodel(batch_x)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/nlp-tutorial/1-1.NNLM/20230106.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     loss\u001b[39m=\u001b[39mcriterion(pred,batch_y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/nlp-tutorial/1-1.NNLM/20230106.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mif\u001b[39;00m (epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m%\u001b[39m\u001b[39m100\u001b[39m\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/nlp-tutorial/1-1.NNLM/20230106.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39mprint\u001b[39m(epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m,loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda3\\envs\\week6\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda3\\envs\\week6\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1164\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1164\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m   1165\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[0;32m   1166\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda3\\envs\\week6\\lib\\site-packages\\torch\\nn\\functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3012\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3013\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3014\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (3) to match target batch_size (2)."
     ]
    }
   ],
   "source": [
    "model=NNLM()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "for epoch in range(100):\n",
    "    for batch_x,batch_y in loader:\n",
    "        pred=model(batch_x)\n",
    "        loss=criterion(pred,batch_y)\n",
    "        \n",
    "        \n",
    "        if (epoch+1)%100==0:\n",
    "            print(epoch+1,loss.item())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 4]' is invalid for input of size 18",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\机器学习\\nlp-tutorial\\1-1.NNLM\\me\\20230106.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/nlp-tutorial/1-1.NNLM/me/20230106.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m pred\u001b[39m=\u001b[39mmodel(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/nlp-tutorial/1-1.NNLM/me/20230106.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(pred)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/nlp-tutorial/1-1.NNLM/me/20230106.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m pred_idx\u001b[39m=\u001b[39mpred\u001b[39m.\u001b[39mmax(\u001b[39m1\u001b[39m,keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32md:\\机器学习\\nlp-tutorial\\1-1.NNLM\\me\\20230106.ipynb Cell 15\u001b[0m in \u001b[0;36mNNLM.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/nlp-tutorial/1-1.NNLM/me/20230106.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m X\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mC(X)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/nlp-tutorial/1-1.NNLM/me/20230106.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# print(X.shape)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/nlp-tutorial/1-1.NNLM/me/20230106.ipynb#X20sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m X\u001b[39m=\u001b[39mX\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,n_step\u001b[39m*\u001b[39;49mm)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/nlp-tutorial/1-1.NNLM/me/20230106.ipynb#X20sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m hidden_out\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mtanh(torch\u001b[39m.\u001b[39mmm(X,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mH)\u001b[39m+\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39md)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/nlp-tutorial/1-1.NNLM/me/20230106.ipynb#X20sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m output\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mmm(X,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW)\u001b[39m+\u001b[39mtorch\u001b[39m.\u001b[39mmm(hidden_out,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mU)\u001b[39m+\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[-1, 4]' is invalid for input of size 18"
     ]
    }
   ],
   "source": [
    "pred=model(input)\n",
    "print(pred)\n",
    "pred_idx=pred.max(1,keepdim=True)[1]\n",
    "print(pred_idx)\n",
    "print([idx2word[idx] for idx in pred_idx.squeeeze()])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.11 ('week6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dc22443302a7e1520fd496d19808d411f95cd97f052bed8809baa60e4c089c3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport math\nimport torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as Data\n\n# S: Symbol that shows starting of decoding input\n# E: Symbol that shows starting of decoding output\n# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\nsentences = [\n    # enc_input      ,     dec_input   ,      dec_output\n    ['ich mochte ein bier P', 'S i want a beer .', 'i want a beer . E'],\n    ['ich mochte ein cola P', 'S i want a coke .', 'i want a coke . E']\n]\n\n# Padding Should be Zero\nsrc_vocab = {'P': 0, 'ich': 1, 'mochte': 2, 'ein': 3, 'bier': 4, 'cola': 5}\nsrc_vocab_size = len(src_vocab)\n\ntgt_vocab = {'P': 0, 'i': 1, 'want': 2, 'a': 3, 'beer': 4, 'coke': 5, 'S': 6, 'E': 7, '.': 8}\nidx2word = {i: w for i, w in enumerate(tgt_vocab)}\ntgt_vocab_size = len(tgt_vocab)\n\nsrc_len = 5  # enc_input max sequence length\ntgt_len = 6  # dec_input(=dec_output) max sequence length\n\ndef make_data(sentences):\n    enc_inputs, dec_inputs, dec_outputs = [], [], []\n    for i in range(len(sentences)):\n        enc_input = [[src_vocab[n] for n in sentences[i][0].split()]]  # [[1, 2, 3, 4, 0], [1, 2, 3, 5, 0]]\n        dec_input = [[tgt_vocab[n] for n in sentences[i][1].split()]]  # [[6, 1, 2, 3, 4, 8], [6, 1, 2, 3, 5, 8]]\n        dec_output = [[tgt_vocab[n] for n in sentences[i][2].split()]]  # [[1, 2, 3, 4, 8, 7], [1, 2, 3, 5, 8, 7]]\n\n        enc_inputs.extend(enc_input)\n        dec_inputs.extend(dec_input)\n        dec_outputs.extend(dec_output)\n\n    return torch.LongTensor(enc_inputs), torch.LongTensor(dec_inputs), torch.LongTensor(dec_outputs)\n\nenc_inputs, dec_inputs, dec_outputs = make_data(sentences)\n\nclass MyDataSet(Data.Dataset):\n    def __init__(self, enc_inputs, dec_inputs, dec_outputs):\n        super(MyDataSet, self).__init__()\n        self.enc_inputs = enc_inputs\n        self.dec_inputs = dec_inputs\n        self.dec_outputs = dec_outputs\n\n    def __len__(self):\n        return self.enc_inputs.shape[0]\n\n    def __getitem__(self, idx):\n        return self.enc_inputs[idx], self.dec_inputs[idx], self.dec_outputs[idx]\n\nloader = Data.DataLoader(MyDataSet(enc_inputs, dec_inputs, dec_outputs), 2, True)\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        '''\n        x: [seq_len, batch_size, d_model]\n        '''\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)\n\n# Transformer Parameters\nd_model = 512  # Embedding Size\nd_ff = 2048  # FeedForward dimension\nn_layers = 6  # number of Encoder of Decoder Layer\nn_heads = 8  # number of heads in Multi-Head Attention\n\nclass myTransformer(nn.Module):\n    def __init__(self):\n        super(myTransformer, self).__init__()\n        self.transformer = nn.Transformer(d_model = d_model, nhead= n_heads,\n                                          num_encoder_layers=n_layers, num_decoder_layers=n_layers,\n                                          dim_feedforward=d_ff)\n        self.src_emb = nn.Embedding(src_vocab_size, d_model)\n        self.pos_emb = PositionalEncoding(d_model)\n        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)\n        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=False).cuda()\n    def forward(self, enc_inputs, dec_inputs):\n        '''\n        enc_inputs: [batch_size, src_len], [1,2,3,4,0]\n        dec_inputs: [batch_size, tgt_len], [6,1,2,3,5,8]\n        '''\n        b, src_len = enc_inputs.shape[0], enc_inputs.shape[1]\n        b, tgt_len = dec_inputs.shape[0], dec_inputs.shape[1]\n\n        src_mask = self.transformer.generate_square_subsequent_mask(src_len).cuda()\n        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt_len).cuda()\n        memory_mask = None\n        src_key_padding_mask = enc_inputs.data.eq(0).cuda()                     # [N,S]\n        tgt_key_padding_mask = dec_inputs.data.eq(0).cuda()                     # [N,T]\n        memory_key_padding_mask = src_key_padding_mask                 # [N,S]\n        # 嵌入向量\n        enc_outputs = self.src_emb(enc_inputs)  # [batch_size, src_len, d_model]\n        enc_outputs = self.pos_emb(enc_outputs.transpose(0, 1)).cuda()  # [ src_len, batch_size, d_model]\n        dec_outputs = self.tgt_emb(dec_inputs)  # [batch_size, tgt_len, d_model]\n        dec_outputs = self.pos_emb(dec_outputs.transpose(0, 1)).cuda()  # [ tgt_len,batch_size, d_model]\n        #送入Transformer\n        dec_outputs  = self.transformer(src= enc_outputs, tgt = dec_outputs, src_mask = None, tgt_mask = tgt_mask,\n                                        memory_mask = None, src_key_padding_mask = src_key_padding_mask,\n                                        tgt_key_padding_mask = tgt_key_padding_mask, memory_key_padding_mask = memory_key_padding_mask)\n\n        # 维度变换\n        dec_logits = self.projection(dec_outputs.transpose(0,1))  # dec_logits: [batch_size, tgt_len, tgt_vocab_size]\n        return dec_logits.view(-1, dec_logits.size(-1)), None, None, None\n\nmodel = myTransformer().cuda()\ncriterion = nn.CrossEntropyLoss(ignore_index=0)\noptimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.99)\n\nfor epoch in range(1000):\n    for enc_inputs, dec_inputs, dec_outputs in loader:\n        '''\n        enc_inputs: [batch_size, src_len]\n        dec_inputs: [batch_size, tgt_len]\n        dec_outputs: [batch_size, tgt_len]\n        '''\n        enc_inputs, dec_inputs, dec_outputs = enc_inputs.cuda(), dec_inputs.cuda(), dec_outputs.cuda()\n        # outputs: [batch_size * tgt_len, tgt_vocab_size]\n        outputs, enc_self_attns, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs)\n        loss = criterion(outputs, dec_outputs.view(-1))\n        print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\ndef greedy_decoder(model, enc_input, start_symbol):\n    \"\"\"\n    For simplicity, a Greedy Decoder is Beam search when K=1. This is necessary for inference as we don't know the\n    target sequence input. Therefore we try to generate the| target input word by word, then feed it into the transformer.\n    Starting Reference: http://nlp.seas.harvard.edu/2018/04/03/attention.html#greedy-decoding\n    :param model: Transformer Model\n    :param enc_input: The encoder input\n    :param start_symbol: The start symbol. In this example it is 'S' which corresponds to index 4\n    :return: The target input\n    \"\"\"\n    #enc_outputs, enc_self_attns = model.encoder(enc_input)\n    dec_input = torch.zeros(1, 0).type_as(enc_input.data)\n    terminal = False\n    next_symbol = start_symbol\n    while not terminal:\n        dec_input = torch.cat([dec_input.detach(),torch.tensor([[next_symbol]],dtype=enc_input.dtype).cuda()],-1)\n        dec_outputs, _, _,_ = model(enc_input, dec_input)\n        #projected = model.projection(dec_outputs)\n        prob = dec_outputs.max(dim=-1, keepdim=False)[1]\n        #print('prob:',prob)      #prob: tensor([1], device='cuda:0')\n        next_word = prob.data[-1]\n        next_symbol = next_word\n        if next_symbol == tgt_vocab[\".\"]:\n            terminal = True\n        print(next_word)\n    return dec_input\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-30T11:13:36.927124Z","iopub.execute_input":"2022-11-30T11:13:36.927505Z","iopub.status.idle":"2022-11-30T11:14:06.133322Z","shell.execute_reply.started":"2022-11-30T11:13:36.927472Z","shell.execute_reply":"2022-11-30T11:14:06.132273Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Epoch: 0001 loss = 2.331379\nEpoch: 0002 loss = 2.212128\nEpoch: 0003 loss = 2.137178\nEpoch: 0004 loss = 2.027811\nEpoch: 0005 loss = 2.085351\nEpoch: 0006 loss = 1.835200\nEpoch: 0007 loss = 2.031692\nEpoch: 0008 loss = 2.021124\nEpoch: 0009 loss = 2.027622\nEpoch: 0010 loss = 1.976684\nEpoch: 0011 loss = 1.924780\nEpoch: 0012 loss = 1.922734\nEpoch: 0013 loss = 1.868059\nEpoch: 0014 loss = 1.826882\nEpoch: 0015 loss = 1.880055\nEpoch: 0016 loss = 1.804686\nEpoch: 0017 loss = 1.862352\nEpoch: 0018 loss = 1.850003\nEpoch: 0019 loss = 1.771620\nEpoch: 0020 loss = 1.860547\nEpoch: 0021 loss = 1.745496\nEpoch: 0022 loss = 1.851793\nEpoch: 0023 loss = 1.725680\nEpoch: 0024 loss = 1.761421\nEpoch: 0025 loss = 1.719893\nEpoch: 0026 loss = 1.796133\nEpoch: 0027 loss = 1.790620\nEpoch: 0028 loss = 1.647845\nEpoch: 0029 loss = 1.656806\nEpoch: 0030 loss = 1.492424\nEpoch: 0031 loss = 1.486343\nEpoch: 0032 loss = 1.606932\nEpoch: 0033 loss = 1.506944\nEpoch: 0034 loss = 1.513952\nEpoch: 0035 loss = 1.402889\nEpoch: 0036 loss = 1.406277\nEpoch: 0037 loss = 1.373958\nEpoch: 0038 loss = 1.154809\nEpoch: 0039 loss = 1.074544\nEpoch: 0040 loss = 1.096959\nEpoch: 0041 loss = 1.013816\nEpoch: 0042 loss = 0.958352\nEpoch: 0043 loss = 0.716670\nEpoch: 0044 loss = 0.811999\nEpoch: 0045 loss = 0.772124\nEpoch: 0046 loss = 0.728936\nEpoch: 0047 loss = 0.618120\nEpoch: 0048 loss = 0.548583\nEpoch: 0049 loss = 0.605994\nEpoch: 0050 loss = 0.650380\nEpoch: 0051 loss = 0.463119\nEpoch: 0052 loss = 0.312135\nEpoch: 0053 loss = 0.323814\nEpoch: 0054 loss = 0.335033\nEpoch: 0055 loss = 0.292594\nEpoch: 0056 loss = 0.224090\nEpoch: 0057 loss = 0.226794\nEpoch: 0058 loss = 0.244952\nEpoch: 0059 loss = 0.206778\nEpoch: 0060 loss = 0.218155\nEpoch: 0061 loss = 0.156551\nEpoch: 0062 loss = 0.176820\nEpoch: 0063 loss = 0.196975\nEpoch: 0064 loss = 0.117494\nEpoch: 0065 loss = 0.085657\nEpoch: 0066 loss = 0.226714\nEpoch: 0067 loss = 0.084661\nEpoch: 0068 loss = 0.061573\nEpoch: 0069 loss = 0.065833\nEpoch: 0070 loss = 0.060152\nEpoch: 0071 loss = 0.124556\nEpoch: 0072 loss = 0.105250\nEpoch: 0073 loss = 0.070510\nEpoch: 0074 loss = 0.055514\nEpoch: 0075 loss = 0.048013\nEpoch: 0076 loss = 0.038923\nEpoch: 0077 loss = 0.035057\nEpoch: 0078 loss = 0.045788\nEpoch: 0079 loss = 0.026625\nEpoch: 0080 loss = 0.017018\nEpoch: 0081 loss = 0.048024\nEpoch: 0082 loss = 0.071596\nEpoch: 0083 loss = 0.059263\nEpoch: 0084 loss = 0.020922\nEpoch: 0085 loss = 0.021249\nEpoch: 0086 loss = 0.014102\nEpoch: 0087 loss = 0.014765\nEpoch: 0088 loss = 0.010501\nEpoch: 0089 loss = 0.007615\nEpoch: 0090 loss = 0.013044\nEpoch: 0091 loss = 0.009560\nEpoch: 0092 loss = 0.012520\nEpoch: 0093 loss = 0.007199\nEpoch: 0094 loss = 0.012006\nEpoch: 0095 loss = 0.022728\nEpoch: 0096 loss = 0.020686\nEpoch: 0097 loss = 0.007831\nEpoch: 0098 loss = 0.021718\nEpoch: 0099 loss = 0.011414\nEpoch: 0100 loss = 0.025388\nEpoch: 0101 loss = 0.014378\nEpoch: 0102 loss = 0.010850\nEpoch: 0103 loss = 0.007099\nEpoch: 0104 loss = 0.008035\nEpoch: 0105 loss = 0.006926\nEpoch: 0106 loss = 0.008560\nEpoch: 0107 loss = 0.006089\nEpoch: 0108 loss = 0.010164\nEpoch: 0109 loss = 0.004234\nEpoch: 0110 loss = 0.006341\nEpoch: 0111 loss = 0.004499\nEpoch: 0112 loss = 0.007480\nEpoch: 0113 loss = 0.006061\nEpoch: 0114 loss = 0.002313\nEpoch: 0115 loss = 0.005023\nEpoch: 0116 loss = 0.001728\nEpoch: 0117 loss = 0.004815\nEpoch: 0118 loss = 0.003337\nEpoch: 0119 loss = 0.004489\nEpoch: 0120 loss = 0.001129\nEpoch: 0121 loss = 0.002182\nEpoch: 0122 loss = 0.001261\nEpoch: 0123 loss = 0.010192\nEpoch: 0124 loss = 0.003448\nEpoch: 0125 loss = 0.005960\nEpoch: 0126 loss = 0.003928\nEpoch: 0127 loss = 0.005226\nEpoch: 0128 loss = 0.005094\nEpoch: 0129 loss = 0.003884\nEpoch: 0130 loss = 0.005270\nEpoch: 0131 loss = 0.003372\nEpoch: 0132 loss = 0.001261\nEpoch: 0133 loss = 0.002967\nEpoch: 0134 loss = 0.001471\nEpoch: 0135 loss = 0.006571\nEpoch: 0136 loss = 0.001823\nEpoch: 0137 loss = 0.002097\nEpoch: 0138 loss = 0.003633\nEpoch: 0139 loss = 0.023254\nEpoch: 0140 loss = 0.001995\nEpoch: 0141 loss = 0.002175\nEpoch: 0142 loss = 0.002009\nEpoch: 0143 loss = 0.002344\nEpoch: 0144 loss = 0.006435\nEpoch: 0145 loss = 0.002097\nEpoch: 0146 loss = 0.015339\nEpoch: 0147 loss = 0.008054\nEpoch: 0148 loss = 0.005311\nEpoch: 0149 loss = 0.011326\nEpoch: 0150 loss = 0.002544\nEpoch: 0151 loss = 0.001219\nEpoch: 0152 loss = 0.000757\nEpoch: 0153 loss = 0.001823\nEpoch: 0154 loss = 0.001474\nEpoch: 0155 loss = 0.006802\nEpoch: 0156 loss = 0.002447\nEpoch: 0157 loss = 0.000884\nEpoch: 0158 loss = 0.001316\nEpoch: 0159 loss = 0.001491\nEpoch: 0160 loss = 0.000601\nEpoch: 0161 loss = 0.000260\nEpoch: 0162 loss = 0.002773\nEpoch: 0163 loss = 0.001543\nEpoch: 0164 loss = 0.000833\nEpoch: 0165 loss = 0.000380\nEpoch: 0166 loss = 0.000793\nEpoch: 0167 loss = 0.000718\nEpoch: 0168 loss = 0.003891\nEpoch: 0169 loss = 0.000152\nEpoch: 0170 loss = 0.009148\nEpoch: 0171 loss = 0.001022\nEpoch: 0172 loss = 0.000671\nEpoch: 0173 loss = 0.000493\nEpoch: 0174 loss = 0.000758\nEpoch: 0175 loss = 0.000465\nEpoch: 0176 loss = 0.009932\nEpoch: 0177 loss = 0.002022\nEpoch: 0178 loss = 0.003302\nEpoch: 0179 loss = 0.000707\nEpoch: 0180 loss = 0.005997\nEpoch: 0181 loss = 0.001200\nEpoch: 0182 loss = 0.001131\nEpoch: 0183 loss = 0.002858\nEpoch: 0184 loss = 0.000705\nEpoch: 0185 loss = 0.000219\nEpoch: 0186 loss = 0.000583\nEpoch: 0187 loss = 0.006621\nEpoch: 0188 loss = 0.001465\nEpoch: 0189 loss = 0.000297\nEpoch: 0190 loss = 0.000180\nEpoch: 0191 loss = 0.003189\nEpoch: 0192 loss = 0.007578\nEpoch: 0193 loss = 0.000341\nEpoch: 0194 loss = 0.002131\nEpoch: 0195 loss = 0.000515\nEpoch: 0196 loss = 0.000180\nEpoch: 0197 loss = 0.000267\nEpoch: 0198 loss = 0.000216\nEpoch: 0199 loss = 0.001801\nEpoch: 0200 loss = 0.000242\nEpoch: 0201 loss = 0.001070\nEpoch: 0202 loss = 0.000503\nEpoch: 0203 loss = 0.000225\nEpoch: 0204 loss = 0.003838\nEpoch: 0205 loss = 0.000324\nEpoch: 0206 loss = 0.000182\nEpoch: 0207 loss = 0.000140\nEpoch: 0208 loss = 0.000255\nEpoch: 0209 loss = 0.000235\nEpoch: 0210 loss = 0.001272\nEpoch: 0211 loss = 0.000135\nEpoch: 0212 loss = 0.000148\nEpoch: 0213 loss = 0.003617\nEpoch: 0214 loss = 0.000820\nEpoch: 0215 loss = 0.000237\nEpoch: 0216 loss = 0.000035\nEpoch: 0217 loss = 0.001592\nEpoch: 0218 loss = 0.000453\nEpoch: 0219 loss = 0.000075\nEpoch: 0220 loss = 0.000092\nEpoch: 0221 loss = 0.000093\nEpoch: 0222 loss = 0.000375\nEpoch: 0223 loss = 0.000110\nEpoch: 0224 loss = 0.000479\nEpoch: 0225 loss = 0.065220\nEpoch: 0226 loss = 0.000040\nEpoch: 0227 loss = 0.000017\nEpoch: 0228 loss = 0.000125\nEpoch: 0229 loss = 0.002687\nEpoch: 0230 loss = 0.001337\nEpoch: 0231 loss = 0.123672\nEpoch: 0232 loss = 0.034288\nEpoch: 0233 loss = 0.043980\nEpoch: 0234 loss = 0.002280\nEpoch: 0235 loss = 0.016226\nEpoch: 0236 loss = 0.009930\nEpoch: 0237 loss = 0.165358\nEpoch: 0238 loss = 0.012376\nEpoch: 0239 loss = 0.004979\nEpoch: 0240 loss = 0.023683\nEpoch: 0241 loss = 0.007154\nEpoch: 0242 loss = 0.043141\nEpoch: 0243 loss = 0.101894\nEpoch: 0244 loss = 0.036814\nEpoch: 0245 loss = 0.000999\nEpoch: 0246 loss = 0.002201\nEpoch: 0247 loss = 0.000810\nEpoch: 0248 loss = 0.006228\nEpoch: 0249 loss = 0.102211\nEpoch: 0250 loss = 0.000955\nEpoch: 0251 loss = 0.001196\nEpoch: 0252 loss = 0.001603\nEpoch: 0253 loss = 0.094479\nEpoch: 0254 loss = 0.033742\nEpoch: 0255 loss = 0.146004\nEpoch: 0256 loss = 0.025397\nEpoch: 0257 loss = 0.058670\nEpoch: 0258 loss = 0.010818\nEpoch: 0259 loss = 0.073904\nEpoch: 0260 loss = 0.002689\nEpoch: 0261 loss = 0.001046\nEpoch: 0262 loss = 0.005738\nEpoch: 0263 loss = 0.001698\nEpoch: 0264 loss = 0.003750\nEpoch: 0265 loss = 0.027712\nEpoch: 0266 loss = 0.007350\nEpoch: 0267 loss = 0.107957\nEpoch: 0268 loss = 0.001172\nEpoch: 0269 loss = 0.000211\nEpoch: 0270 loss = 0.000701\nEpoch: 0271 loss = 0.024751\nEpoch: 0272 loss = 0.008294\nEpoch: 0273 loss = 0.072044\nEpoch: 0274 loss = 0.000619\nEpoch: 0275 loss = 0.010573\nEpoch: 0276 loss = 0.015072\nEpoch: 0277 loss = 0.006622\nEpoch: 0278 loss = 0.005238\nEpoch: 0279 loss = 0.034365\nEpoch: 0280 loss = 0.015593\nEpoch: 0281 loss = 0.044673\nEpoch: 0282 loss = 0.001012\nEpoch: 0283 loss = 0.011533\nEpoch: 0284 loss = 0.538049\nEpoch: 0285 loss = 0.000476\nEpoch: 0286 loss = 0.002003\nEpoch: 0287 loss = 0.000113\nEpoch: 0288 loss = 0.000900\nEpoch: 0289 loss = 0.001961\nEpoch: 0290 loss = 0.000294\nEpoch: 0291 loss = 0.002109\nEpoch: 0292 loss = 0.001247\nEpoch: 0293 loss = 0.000575\nEpoch: 0294 loss = 0.025105\nEpoch: 0295 loss = 0.009807\nEpoch: 0296 loss = 0.004621\nEpoch: 0297 loss = 0.030204\nEpoch: 0298 loss = 0.026173\nEpoch: 0299 loss = 0.039494\nEpoch: 0300 loss = 0.000209\nEpoch: 0301 loss = 0.000178\nEpoch: 0302 loss = 0.000029\nEpoch: 0303 loss = 0.000174\nEpoch: 0304 loss = 0.000420\nEpoch: 0305 loss = 0.000020\nEpoch: 0306 loss = 0.000575\nEpoch: 0307 loss = 0.000019\nEpoch: 0308 loss = 0.000059\nEpoch: 0309 loss = 0.000493\nEpoch: 0310 loss = 0.002375\nEpoch: 0311 loss = 0.013649\nEpoch: 0312 loss = 0.081257\nEpoch: 0313 loss = 0.015963\nEpoch: 0314 loss = 0.010424\nEpoch: 0315 loss = 0.000370\nEpoch: 0316 loss = 0.000309\nEpoch: 0317 loss = 0.002652\nEpoch: 0318 loss = 0.004735\nEpoch: 0319 loss = 0.002588\nEpoch: 0320 loss = 0.001054\nEpoch: 0321 loss = 0.001069\nEpoch: 0322 loss = 0.028585\nEpoch: 0323 loss = 0.003863\nEpoch: 0324 loss = 0.001885\nEpoch: 0325 loss = 0.081016\nEpoch: 0326 loss = 0.001411\nEpoch: 0327 loss = 0.063678\nEpoch: 0328 loss = 0.002702\nEpoch: 0329 loss = 0.001228\nEpoch: 0330 loss = 0.000775\nEpoch: 0331 loss = 0.072882\nEpoch: 0332 loss = 0.000379\nEpoch: 0333 loss = 0.001545\nEpoch: 0334 loss = 0.004043\nEpoch: 0335 loss = 0.253426\nEpoch: 0336 loss = 0.009560\nEpoch: 0337 loss = 0.002049\nEpoch: 0338 loss = 0.017700\nEpoch: 0339 loss = 0.005398\nEpoch: 0340 loss = 0.013127\nEpoch: 0341 loss = 0.039842\nEpoch: 0342 loss = 0.014456\nEpoch: 0343 loss = 0.005818\nEpoch: 0344 loss = 0.017785\nEpoch: 0345 loss = 0.002874\nEpoch: 0346 loss = 0.032891\nEpoch: 0347 loss = 0.004981\nEpoch: 0348 loss = 0.005322\nEpoch: 0349 loss = 0.054948\nEpoch: 0350 loss = 0.002624\nEpoch: 0351 loss = 0.000957\nEpoch: 0352 loss = 0.000761\nEpoch: 0353 loss = 0.002716\nEpoch: 0354 loss = 0.004258\nEpoch: 0355 loss = 0.004503\nEpoch: 0356 loss = 0.000913\nEpoch: 0357 loss = 0.001585\nEpoch: 0358 loss = 0.028748\nEpoch: 0359 loss = 0.004986\nEpoch: 0360 loss = 0.043892\nEpoch: 0361 loss = 0.010974\nEpoch: 0362 loss = 0.020553\nEpoch: 0363 loss = 0.001214\nEpoch: 0364 loss = 0.021177\nEpoch: 0365 loss = 0.005814\nEpoch: 0366 loss = 0.003580\nEpoch: 0367 loss = 0.004508\nEpoch: 0368 loss = 0.013888\nEpoch: 0369 loss = 0.024625\nEpoch: 0370 loss = 0.044664\nEpoch: 0371 loss = 0.011346\nEpoch: 0372 loss = 0.012265\nEpoch: 0373 loss = 0.011501\nEpoch: 0374 loss = 0.001215\nEpoch: 0375 loss = 0.002591\nEpoch: 0376 loss = 0.003071\nEpoch: 0377 loss = 0.006158\nEpoch: 0378 loss = 0.001725\nEpoch: 0379 loss = 0.022022\nEpoch: 0380 loss = 0.022509\nEpoch: 0381 loss = 0.033503\nEpoch: 0382 loss = 0.065158\nEpoch: 0383 loss = 0.028645\nEpoch: 0384 loss = 0.010616\nEpoch: 0385 loss = 0.001540\nEpoch: 0386 loss = 0.005561\nEpoch: 0387 loss = 0.008588\nEpoch: 0388 loss = 0.001187\nEpoch: 0389 loss = 0.009630\nEpoch: 0390 loss = 0.016105\nEpoch: 0391 loss = 0.031192\nEpoch: 0392 loss = 0.001151\nEpoch: 0393 loss = 0.004803\nEpoch: 0394 loss = 0.003797\nEpoch: 0395 loss = 0.021249\nEpoch: 0396 loss = 0.002532\nEpoch: 0397 loss = 0.013886\nEpoch: 0398 loss = 0.006963\nEpoch: 0399 loss = 0.014790\nEpoch: 0400 loss = 0.019368\nEpoch: 0401 loss = 0.003811\nEpoch: 0402 loss = 0.001665\nEpoch: 0403 loss = 0.001032\nEpoch: 0404 loss = 0.002048\nEpoch: 0405 loss = 0.002432\nEpoch: 0406 loss = 0.067873\nEpoch: 0407 loss = 0.017766\nEpoch: 0408 loss = 0.001982\nEpoch: 0409 loss = 0.002943\nEpoch: 0410 loss = 0.001347\nEpoch: 0411 loss = 0.007190\nEpoch: 0412 loss = 0.001512\nEpoch: 0413 loss = 0.001408\nEpoch: 0414 loss = 0.001371\nEpoch: 0415 loss = 0.003753\nEpoch: 0416 loss = 0.002627\nEpoch: 0417 loss = 0.004176\nEpoch: 0418 loss = 0.005071\nEpoch: 0419 loss = 0.013238\nEpoch: 0420 loss = 0.002282\nEpoch: 0421 loss = 0.000507\nEpoch: 0422 loss = 0.000631\nEpoch: 0423 loss = 0.000135\nEpoch: 0424 loss = 0.000175\nEpoch: 0425 loss = 0.000062\nEpoch: 0426 loss = 0.000643\nEpoch: 0427 loss = 0.000042\nEpoch: 0428 loss = 0.000155\nEpoch: 0429 loss = 0.000509\nEpoch: 0430 loss = 0.001788\nEpoch: 0431 loss = 0.000989\nEpoch: 0432 loss = 0.001573\nEpoch: 0433 loss = 0.027930\nEpoch: 0434 loss = 0.001047\nEpoch: 0435 loss = 0.000421\nEpoch: 0436 loss = 0.003469\nEpoch: 0437 loss = 0.005617\nEpoch: 0438 loss = 0.001342\nEpoch: 0439 loss = 0.000118\nEpoch: 0440 loss = 0.156973\nEpoch: 0441 loss = 0.000281\nEpoch: 0442 loss = 0.000077\nEpoch: 0443 loss = 0.000044\nEpoch: 0444 loss = 0.000017\nEpoch: 0445 loss = 0.000746\nEpoch: 0446 loss = 0.000375\nEpoch: 0447 loss = 0.005076\nEpoch: 0448 loss = 0.142434\nEpoch: 0449 loss = 0.001027\nEpoch: 0450 loss = 0.004298\nEpoch: 0451 loss = 0.003872\nEpoch: 0452 loss = 0.004651\nEpoch: 0453 loss = 0.016847\nEpoch: 0454 loss = 0.004275\nEpoch: 0455 loss = 0.022618\nEpoch: 0456 loss = 0.653410\nEpoch: 0457 loss = 0.000195\nEpoch: 0458 loss = 0.000339\nEpoch: 0459 loss = 0.000569\nEpoch: 0460 loss = 0.000143\nEpoch: 0461 loss = 0.000018\nEpoch: 0462 loss = 0.000251\nEpoch: 0463 loss = 0.000326\nEpoch: 0464 loss = 0.137158\nEpoch: 0465 loss = 0.000105\nEpoch: 0466 loss = 0.002775\nEpoch: 0467 loss = 0.000088\nEpoch: 0468 loss = 0.001821\nEpoch: 0469 loss = 0.006006\nEpoch: 0470 loss = 0.000008\nEpoch: 0471 loss = 0.000044\nEpoch: 0472 loss = 0.000133\nEpoch: 0473 loss = 0.000937\nEpoch: 0474 loss = 0.000162\nEpoch: 0475 loss = 0.007891\nEpoch: 0476 loss = 0.008225\nEpoch: 0477 loss = 0.001361\nEpoch: 0478 loss = 0.005768\nEpoch: 0479 loss = 0.972445\nEpoch: 0480 loss = 0.004136\nEpoch: 0481 loss = 0.002353\nEpoch: 0482 loss = 0.002587\nEpoch: 0483 loss = 0.000292\nEpoch: 0484 loss = 0.000929\nEpoch: 0485 loss = 0.040035\nEpoch: 0486 loss = 0.071892\nEpoch: 0487 loss = 0.272992\nEpoch: 0488 loss = 0.011502\nEpoch: 0489 loss = 0.002681\nEpoch: 0490 loss = 0.007678\nEpoch: 0491 loss = 0.000844\nEpoch: 0492 loss = 0.001557\nEpoch: 0493 loss = 0.000812\nEpoch: 0494 loss = 0.000378\nEpoch: 0495 loss = 0.005850\nEpoch: 0496 loss = 0.000966\nEpoch: 0497 loss = 0.014001\nEpoch: 0498 loss = 0.155387\nEpoch: 0499 loss = 0.032433\nEpoch: 0500 loss = 0.046731\nEpoch: 0501 loss = 0.000152\nEpoch: 0502 loss = 0.000135\nEpoch: 0503 loss = 0.000180\nEpoch: 0504 loss = 0.000156\nEpoch: 0505 loss = 0.000208\nEpoch: 0506 loss = 0.000020\nEpoch: 0507 loss = 0.000040\nEpoch: 0508 loss = 0.000297\nEpoch: 0509 loss = 0.000059\nEpoch: 0510 loss = 0.080227\nEpoch: 0511 loss = 0.000054\nEpoch: 0512 loss = 0.003726\nEpoch: 0513 loss = 0.000029\nEpoch: 0514 loss = 0.000112\nEpoch: 0515 loss = 0.000019\nEpoch: 0516 loss = 0.000024\nEpoch: 0517 loss = 0.000073\nEpoch: 0518 loss = 0.000212\nEpoch: 0519 loss = 0.086175\nEpoch: 0520 loss = 0.000016\nEpoch: 0521 loss = 0.011873\nEpoch: 0522 loss = 0.000107\nEpoch: 0523 loss = 0.000027\nEpoch: 0524 loss = 0.000148\nEpoch: 0525 loss = 0.000081\nEpoch: 0526 loss = 0.000487\nEpoch: 0527 loss = 0.019280\nEpoch: 0528 loss = 0.000183\nEpoch: 0529 loss = 0.000356\nEpoch: 0530 loss = 0.017322\nEpoch: 0531 loss = 0.001584\nEpoch: 0532 loss = 0.128780\nEpoch: 0533 loss = 0.026337\nEpoch: 0534 loss = 0.003909\nEpoch: 0535 loss = 0.007067\nEpoch: 0536 loss = 0.006268\nEpoch: 0537 loss = 0.051115\nEpoch: 0538 loss = 0.010285\nEpoch: 0539 loss = 0.000567\nEpoch: 0540 loss = 0.000129\nEpoch: 0541 loss = 0.000047\nEpoch: 0542 loss = 0.000124\nEpoch: 0543 loss = 0.000077\nEpoch: 0544 loss = 0.000164\nEpoch: 0545 loss = 0.000210\nEpoch: 0546 loss = 0.006142\nEpoch: 0547 loss = 0.000129\nEpoch: 0548 loss = 0.032965\nEpoch: 0549 loss = 0.003793\nEpoch: 0550 loss = 0.007949\nEpoch: 0551 loss = 0.008194\nEpoch: 0552 loss = 0.134735\nEpoch: 0553 loss = 0.001296\nEpoch: 0554 loss = 0.006208\nEpoch: 0555 loss = 0.021518\nEpoch: 0556 loss = 0.001381\nEpoch: 0557 loss = 0.000681\nEpoch: 0558 loss = 0.000250\nEpoch: 0559 loss = 0.000679\nEpoch: 0560 loss = 0.005315\nEpoch: 0561 loss = 0.003041\nEpoch: 0562 loss = 0.002726\nEpoch: 0563 loss = 0.002912\nEpoch: 0564 loss = 0.001501\nEpoch: 0565 loss = 0.003156\nEpoch: 0566 loss = 0.012487\nEpoch: 0567 loss = 0.037746\nEpoch: 0568 loss = 0.065972\nEpoch: 0569 loss = 0.040130\nEpoch: 0570 loss = 0.021818\nEpoch: 0571 loss = 0.000369\nEpoch: 0572 loss = 0.001103\nEpoch: 0573 loss = 0.055042\nEpoch: 0574 loss = 0.036630\nEpoch: 0575 loss = 0.117173\nEpoch: 0576 loss = 0.001496\nEpoch: 0577 loss = 0.000125\nEpoch: 0578 loss = 0.000927\nEpoch: 0579 loss = 0.000263\nEpoch: 0580 loss = 0.000930\nEpoch: 0581 loss = 0.007539\nEpoch: 0582 loss = 0.001977\nEpoch: 0583 loss = 0.001933\nEpoch: 0584 loss = 0.061278\nEpoch: 0585 loss = 0.009801\nEpoch: 0586 loss = 0.014665\nEpoch: 0587 loss = 0.048528\nEpoch: 0588 loss = 0.044845\nEpoch: 0589 loss = 0.124559\nEpoch: 0590 loss = 0.047791\nEpoch: 0591 loss = 0.072699\nEpoch: 0592 loss = 0.004545\nEpoch: 0593 loss = 0.021604\nEpoch: 0594 loss = 0.002716\nEpoch: 0595 loss = 0.005228\nEpoch: 0596 loss = 0.003410\nEpoch: 0597 loss = 0.000564\nEpoch: 0598 loss = 0.007700\nEpoch: 0599 loss = 0.100793\nEpoch: 0600 loss = 0.000198\nEpoch: 0601 loss = 0.000487\nEpoch: 0602 loss = 0.000262\nEpoch: 0603 loss = 0.003923\nEpoch: 0604 loss = 0.001880\nEpoch: 0605 loss = 0.001496\nEpoch: 0606 loss = 0.000847\nEpoch: 0607 loss = 0.013491\nEpoch: 0608 loss = 0.000205\nEpoch: 0609 loss = 0.002299\nEpoch: 0610 loss = 0.033612\nEpoch: 0611 loss = 0.014749\nEpoch: 0612 loss = 0.001393\nEpoch: 0613 loss = 0.001332\nEpoch: 0614 loss = 0.000749\nEpoch: 0615 loss = 0.000889\nEpoch: 0616 loss = 0.058998\nEpoch: 0617 loss = 0.000119\nEpoch: 0618 loss = 0.000216\nEpoch: 0619 loss = 0.000975\nEpoch: 0620 loss = 0.001159\nEpoch: 0621 loss = 0.000250\nEpoch: 0622 loss = 0.001125\nEpoch: 0623 loss = 0.000129\nEpoch: 0624 loss = 0.000697\nEpoch: 0625 loss = 0.021983\nEpoch: 0626 loss = 0.000214\nEpoch: 0627 loss = 0.001010\nEpoch: 0628 loss = 0.000836\nEpoch: 0629 loss = 0.022881\nEpoch: 0630 loss = 0.001424\nEpoch: 0631 loss = 0.000962\nEpoch: 0632 loss = 0.000198\nEpoch: 0633 loss = 0.000409\nEpoch: 0634 loss = 0.000294\nEpoch: 0635 loss = 0.000248\nEpoch: 0636 loss = 0.001266\nEpoch: 0637 loss = 0.000097\nEpoch: 0638 loss = 0.000424\nEpoch: 0639 loss = 0.008716\nEpoch: 0640 loss = 0.000860\nEpoch: 0641 loss = 0.000257\nEpoch: 0642 loss = 0.000225\nEpoch: 0643 loss = 0.000534\nEpoch: 0644 loss = 0.000220\nEpoch: 0645 loss = 0.000203\nEpoch: 0646 loss = 0.009947\nEpoch: 0647 loss = 0.000270\nEpoch: 0648 loss = 0.000106\nEpoch: 0649 loss = 0.000184\nEpoch: 0650 loss = 0.000707\nEpoch: 0651 loss = 0.000370\nEpoch: 0652 loss = 0.000058\nEpoch: 0653 loss = 0.000116\nEpoch: 0654 loss = 0.000050\nEpoch: 0655 loss = 0.000116\nEpoch: 0656 loss = 0.000091\nEpoch: 0657 loss = 0.000440\nEpoch: 0658 loss = 0.000349\nEpoch: 0659 loss = 0.000079\nEpoch: 0660 loss = 0.000096\nEpoch: 0661 loss = 0.000311\nEpoch: 0662 loss = 0.000450\nEpoch: 0663 loss = 0.000208\nEpoch: 0664 loss = 0.005707\nEpoch: 0665 loss = 0.000194\nEpoch: 0666 loss = 0.000115\nEpoch: 0667 loss = 0.000134\nEpoch: 0668 loss = 0.001303\nEpoch: 0669 loss = 0.000215\nEpoch: 0670 loss = 0.000454\nEpoch: 0671 loss = 0.000554\nEpoch: 0672 loss = 0.000058\nEpoch: 0673 loss = 0.000158\nEpoch: 0674 loss = 0.000302\nEpoch: 0675 loss = 0.007639\nEpoch: 0676 loss = 0.063012\nEpoch: 0677 loss = 0.013973\nEpoch: 0678 loss = 0.004706\nEpoch: 0679 loss = 0.000032\nEpoch: 0680 loss = 0.000190\nEpoch: 0681 loss = 0.000139\nEpoch: 0682 loss = 0.000346\nEpoch: 0683 loss = 0.000138\nEpoch: 0684 loss = 0.000030\nEpoch: 0685 loss = 0.000295\nEpoch: 0686 loss = 0.000367\nEpoch: 0687 loss = 0.001780\nEpoch: 0688 loss = 0.000138\nEpoch: 0689 loss = 0.000486\nEpoch: 0690 loss = 0.001265\nEpoch: 0691 loss = 0.000488\nEpoch: 0692 loss = 0.001623\nEpoch: 0693 loss = 0.000445\nEpoch: 0694 loss = 0.001960\nEpoch: 0695 loss = 0.000052\nEpoch: 0696 loss = 0.000625\nEpoch: 0697 loss = 0.000246\nEpoch: 0698 loss = 0.000765\nEpoch: 0699 loss = 0.000693\nEpoch: 0700 loss = 0.000394\nEpoch: 0701 loss = 0.004071\nEpoch: 0702 loss = 0.002390\nEpoch: 0703 loss = 0.002304\nEpoch: 0704 loss = 0.006617\nEpoch: 0705 loss = 0.009123\nEpoch: 0706 loss = 0.000266\nEpoch: 0707 loss = 0.000690\nEpoch: 0708 loss = 0.007592\nEpoch: 0709 loss = 0.000638\nEpoch: 0710 loss = 0.000413\nEpoch: 0711 loss = 0.004064\nEpoch: 0712 loss = 0.000757\nEpoch: 0713 loss = 0.007547\nEpoch: 0714 loss = 0.018724\nEpoch: 0715 loss = 0.013488\nEpoch: 0716 loss = 0.028612\nEpoch: 0717 loss = 0.002278\nEpoch: 0718 loss = 0.004072\nEpoch: 0719 loss = 0.001957\nEpoch: 0720 loss = 0.001636\nEpoch: 0721 loss = 0.000231\nEpoch: 0722 loss = 0.000796\nEpoch: 0723 loss = 0.000820\nEpoch: 0724 loss = 0.000463\nEpoch: 0725 loss = 0.000540\nEpoch: 0726 loss = 0.001211\nEpoch: 0727 loss = 0.000134\nEpoch: 0728 loss = 0.001248\nEpoch: 0729 loss = 0.000815\nEpoch: 0730 loss = 0.000115\nEpoch: 0731 loss = 0.000363\nEpoch: 0732 loss = 0.000238\nEpoch: 0733 loss = 0.000637\nEpoch: 0734 loss = 0.015528\nEpoch: 0735 loss = 0.000351\nEpoch: 0736 loss = 0.001691\nEpoch: 0737 loss = 0.000130\nEpoch: 0738 loss = 0.001058\nEpoch: 0739 loss = 0.000018\nEpoch: 0740 loss = 0.000481\nEpoch: 0741 loss = 0.004174\nEpoch: 0742 loss = 0.000203\nEpoch: 0743 loss = 0.000981\nEpoch: 0744 loss = 0.000454\nEpoch: 0745 loss = 0.001692\nEpoch: 0746 loss = 0.000083\nEpoch: 0747 loss = 0.001729\nEpoch: 0748 loss = 0.000414\nEpoch: 0749 loss = 0.000074\nEpoch: 0750 loss = 0.000080\nEpoch: 0751 loss = 0.000031\nEpoch: 0752 loss = 0.007333\nEpoch: 0753 loss = 0.002864\nEpoch: 0754 loss = 0.000158\nEpoch: 0755 loss = 0.000085\nEpoch: 0756 loss = 0.000185\nEpoch: 0757 loss = 0.000033\nEpoch: 0758 loss = 0.000025\nEpoch: 0759 loss = 0.000237\nEpoch: 0760 loss = 0.000076\nEpoch: 0761 loss = 0.000192\nEpoch: 0762 loss = 0.002073\nEpoch: 0763 loss = 0.000024\nEpoch: 0764 loss = 0.002198\nEpoch: 0765 loss = 0.000065\nEpoch: 0766 loss = 0.000048\nEpoch: 0767 loss = 0.000020\nEpoch: 0768 loss = 0.000259\nEpoch: 0769 loss = 0.002743\nEpoch: 0770 loss = 0.000597\nEpoch: 0771 loss = 0.003805\nEpoch: 0772 loss = 0.000027\nEpoch: 0773 loss = 0.000626\nEpoch: 0774 loss = 0.000337\nEpoch: 0775 loss = 0.000227\nEpoch: 0776 loss = 0.000021\nEpoch: 0777 loss = 0.000468\nEpoch: 0778 loss = 0.000085\nEpoch: 0779 loss = 0.000124\nEpoch: 0780 loss = 0.000067\nEpoch: 0781 loss = 0.000332\nEpoch: 0782 loss = 0.000124\nEpoch: 0783 loss = 0.000033\nEpoch: 0784 loss = 0.001132\nEpoch: 0785 loss = 0.000089\nEpoch: 0786 loss = 0.001276\nEpoch: 0787 loss = 0.000034\nEpoch: 0788 loss = 0.000119\nEpoch: 0789 loss = 0.000881\nEpoch: 0790 loss = 0.000101\nEpoch: 0791 loss = 0.000064\nEpoch: 0792 loss = 0.000029\nEpoch: 0793 loss = 0.000021\nEpoch: 0794 loss = 0.000038\nEpoch: 0795 loss = 0.000190\nEpoch: 0796 loss = 0.000075\nEpoch: 0797 loss = 0.000245\nEpoch: 0798 loss = 0.000151\nEpoch: 0799 loss = 0.001006\nEpoch: 0800 loss = 0.000058\nEpoch: 0801 loss = 0.000012\nEpoch: 0802 loss = 0.000017\nEpoch: 0803 loss = 0.000024\nEpoch: 0804 loss = 0.000077\nEpoch: 0805 loss = 0.000045\nEpoch: 0806 loss = 0.000048\nEpoch: 0807 loss = 0.000036\nEpoch: 0808 loss = 0.000040\nEpoch: 0809 loss = 0.000013\nEpoch: 0810 loss = 0.000062\nEpoch: 0811 loss = 0.000184\nEpoch: 0812 loss = 0.000033\nEpoch: 0813 loss = 0.000051\nEpoch: 0814 loss = 0.000041\nEpoch: 0815 loss = 0.000010\nEpoch: 0816 loss = 0.000048\nEpoch: 0817 loss = 0.000063\nEpoch: 0818 loss = 0.000027\nEpoch: 0819 loss = 0.000247\nEpoch: 0820 loss = 0.000083\nEpoch: 0821 loss = 0.000041\nEpoch: 0822 loss = 0.000031\nEpoch: 0823 loss = 0.000006\nEpoch: 0824 loss = 0.000016\nEpoch: 0825 loss = 0.000019\nEpoch: 0826 loss = 0.000127\nEpoch: 0827 loss = 0.000016\nEpoch: 0828 loss = 0.000002\nEpoch: 0829 loss = 0.000105\nEpoch: 0830 loss = 0.000052\nEpoch: 0831 loss = 0.000406\nEpoch: 0832 loss = 0.000033\nEpoch: 0833 loss = 0.000064\nEpoch: 0834 loss = 0.000031\nEpoch: 0835 loss = 0.000181\nEpoch: 0836 loss = 0.000018\nEpoch: 0837 loss = 0.000071\nEpoch: 0838 loss = 0.000010\nEpoch: 0839 loss = 0.000013\nEpoch: 0840 loss = 0.000010\nEpoch: 0841 loss = 0.000013\nEpoch: 0842 loss = 0.000018\nEpoch: 0843 loss = 0.000006\nEpoch: 0844 loss = 0.000011\nEpoch: 0845 loss = 0.000025\nEpoch: 0846 loss = 0.000013\nEpoch: 0847 loss = 0.000020\nEpoch: 0848 loss = 0.000013\nEpoch: 0849 loss = 0.000015\nEpoch: 0850 loss = 0.000011\nEpoch: 0851 loss = 0.000006\nEpoch: 0852 loss = 0.000107\nEpoch: 0853 loss = 0.000011\nEpoch: 0854 loss = 0.000011\nEpoch: 0855 loss = 0.000016\nEpoch: 0856 loss = 0.000035\nEpoch: 0857 loss = 0.000045\nEpoch: 0858 loss = 0.000322\nEpoch: 0859 loss = 0.000016\nEpoch: 0860 loss = 0.000019\nEpoch: 0861 loss = 0.000089\nEpoch: 0862 loss = 0.000038\nEpoch: 0863 loss = 0.000005\nEpoch: 0864 loss = 0.000023\nEpoch: 0865 loss = 0.000082\nEpoch: 0866 loss = 0.000010\nEpoch: 0867 loss = 0.000053\nEpoch: 0868 loss = 0.000060\nEpoch: 0869 loss = 0.000041\nEpoch: 0870 loss = 0.000025\nEpoch: 0871 loss = 0.000198\nEpoch: 0872 loss = 0.000120\nEpoch: 0873 loss = 0.000013\nEpoch: 0874 loss = 0.000017\nEpoch: 0875 loss = 0.000761\nEpoch: 0876 loss = 0.000052\nEpoch: 0877 loss = 0.000011\nEpoch: 0878 loss = 0.000161\nEpoch: 0879 loss = 0.000018\nEpoch: 0880 loss = 0.000080\nEpoch: 0881 loss = 0.000019\nEpoch: 0882 loss = 0.000091\nEpoch: 0883 loss = 0.000107\nEpoch: 0884 loss = 0.000035\nEpoch: 0885 loss = 0.000038\nEpoch: 0886 loss = 0.000012\nEpoch: 0887 loss = 0.000029\nEpoch: 0888 loss = 0.000031\nEpoch: 0889 loss = 0.000109\nEpoch: 0890 loss = 0.000019\nEpoch: 0891 loss = 0.000147\nEpoch: 0892 loss = 0.000221\nEpoch: 0893 loss = 0.000008\nEpoch: 0894 loss = 0.000312\nEpoch: 0895 loss = 0.000185\nEpoch: 0896 loss = 0.000070\nEpoch: 0897 loss = 0.000048\nEpoch: 0898 loss = 0.000007\nEpoch: 0899 loss = 0.000023\nEpoch: 0900 loss = 0.000003\nEpoch: 0901 loss = 0.000029\nEpoch: 0902 loss = 0.000063\nEpoch: 0903 loss = 0.000001\nEpoch: 0904 loss = 0.000026\nEpoch: 0905 loss = 0.000006\nEpoch: 0906 loss = 0.000027\nEpoch: 0907 loss = 0.000018\nEpoch: 0908 loss = 0.000003\nEpoch: 0909 loss = 0.000059\nEpoch: 0910 loss = 0.000032\nEpoch: 0911 loss = 0.000014\nEpoch: 0912 loss = 0.000021\nEpoch: 0913 loss = 0.000012\nEpoch: 0914 loss = 0.000005\nEpoch: 0915 loss = 0.000011\nEpoch: 0916 loss = 0.000043\nEpoch: 0917 loss = 0.013295\nEpoch: 0918 loss = 0.000025\nEpoch: 0919 loss = 0.000018\nEpoch: 0920 loss = 0.000011\nEpoch: 0921 loss = 0.000009\nEpoch: 0922 loss = 0.000037\nEpoch: 0923 loss = 0.000015\nEpoch: 0924 loss = 0.000186\nEpoch: 0925 loss = 0.000008\nEpoch: 0926 loss = 0.000134\nEpoch: 0927 loss = 0.000002\nEpoch: 0928 loss = 0.000005\nEpoch: 0929 loss = 0.000024\nEpoch: 0930 loss = 0.000010\nEpoch: 0931 loss = 0.000008\nEpoch: 0932 loss = 0.000195\nEpoch: 0933 loss = 0.000007\nEpoch: 0934 loss = 0.000019\nEpoch: 0935 loss = 0.000003\nEpoch: 0936 loss = 0.000027\nEpoch: 0937 loss = 0.000004\nEpoch: 0938 loss = 0.001196\nEpoch: 0939 loss = 0.000003\nEpoch: 0940 loss = 0.000006\nEpoch: 0941 loss = 0.000010\nEpoch: 0942 loss = 0.000037\nEpoch: 0943 loss = 0.000013\nEpoch: 0944 loss = 0.000002\nEpoch: 0945 loss = 0.000003\nEpoch: 0946 loss = 0.000002\nEpoch: 0947 loss = 0.000029\nEpoch: 0948 loss = 0.000016\nEpoch: 0949 loss = 0.000009\nEpoch: 0950 loss = 0.000328\nEpoch: 0951 loss = 0.000002\nEpoch: 0952 loss = 0.000010\nEpoch: 0953 loss = 0.000010\nEpoch: 0954 loss = 0.000002\nEpoch: 0955 loss = 0.000192\nEpoch: 0956 loss = 0.000018\nEpoch: 0957 loss = 0.000003\nEpoch: 0958 loss = 0.000071\nEpoch: 0959 loss = 0.000004\nEpoch: 0960 loss = 0.000061\nEpoch: 0961 loss = 0.000022\nEpoch: 0962 loss = 0.000049\nEpoch: 0963 loss = 0.002288\nEpoch: 0964 loss = 0.000004\nEpoch: 0965 loss = 0.000005\nEpoch: 0966 loss = 0.000021\nEpoch: 0967 loss = 0.000017\nEpoch: 0968 loss = 0.000005\nEpoch: 0969 loss = 0.000011\nEpoch: 0970 loss = 0.000008\nEpoch: 0971 loss = 0.000006\nEpoch: 0972 loss = 0.000015\nEpoch: 0973 loss = 0.000005\nEpoch: 0974 loss = 0.000106\nEpoch: 0975 loss = 0.000107\nEpoch: 0976 loss = 0.000019\nEpoch: 0977 loss = 0.000056\nEpoch: 0978 loss = 0.000001\nEpoch: 0979 loss = 0.000011\nEpoch: 0980 loss = 0.000007\nEpoch: 0981 loss = 0.000006\nEpoch: 0982 loss = 0.000004\nEpoch: 0983 loss = 0.000008\nEpoch: 0984 loss = 0.000032\nEpoch: 0985 loss = 0.000005\nEpoch: 0986 loss = 0.000001\nEpoch: 0987 loss = 0.000077\nEpoch: 0988 loss = 0.000009\nEpoch: 0989 loss = 0.000004\nEpoch: 0990 loss = 0.000017\nEpoch: 0991 loss = 0.000002\nEpoch: 0992 loss = 0.000003\nEpoch: 0993 loss = 0.000034\nEpoch: 0994 loss = 0.000003\nEpoch: 0995 loss = 0.000015\nEpoch: 0996 loss = 0.000003\nEpoch: 0997 loss = 0.000032\nEpoch: 0998 loss = 0.000002\nEpoch: 0999 loss = 0.000017\nEpoch: 1000 loss = 0.000032\n","output_type":"stream"}]},{"cell_type":"code","source":"# Test\nenc_inputs, _, _ = next(iter(loader))\nenc_inputs = enc_inputs.cuda()\nfor i in range(len(enc_inputs)):\n    greedy_dec_input = greedy_decoder(model, enc_inputs[i].view(1, -1), start_symbol=tgt_vocab[\"S\"])\n    predict, _, _, _ = model(enc_inputs[i].view(1, -1), greedy_dec_input)\n    predict = predict.data.max(1, keepdim=True)[1]\n    print(enc_inputs[i], '->', [idx2word[n.item()] for n in predict.squeeze()])","metadata":{"execution":{"iopub.status.busy":"2022-11-30T11:14:06.136657Z","iopub.execute_input":"2022-11-30T11:14:06.136940Z","iopub.status.idle":"2022-11-30T11:14:06.285802Z","shell.execute_reply.started":"2022-11-30T11:14:06.136914Z","shell.execute_reply":"2022-11-30T11:14:06.284780Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"tensor(1, device='cuda:0')\ntensor(2, device='cuda:0')\ntensor(3, device='cuda:0')\ntensor(5, device='cuda:0')\ntensor(8, device='cuda:0')\ntensor([1, 2, 3, 5, 0], device='cuda:0') -> ['i', 'want', 'a', 'coke', '.']\ntensor(1, device='cuda:0')\ntensor(2, device='cuda:0')\ntensor(3, device='cuda:0')\ntensor(4, device='cuda:0')\ntensor(8, device='cuda:0')\ntensor([1, 2, 3, 4, 0], device='cuda:0') -> ['i', 'want', 'a', 'beer', '.']\n","output_type":"stream"}]}]}